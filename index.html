<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Ian Osband - Homepage</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>

        <!-- MathJax options -->
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true
            }
          });
        </script>

        <!-- Google tracking -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-65368616-1', 'auto');
          ga('send', 'pageview');
        </script>

        <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt">
          <h1><strong><a href="/index.html">OpenAI</a></strong> Reinforcement Learning<br>
          </h1>
				<nav id="nav">
					<ul>
						<li><a href="/">Home</a></li>
            <li><a href="/research.html">Research</a></li>
            <li><a href="/blog.html">Blog</a></li>
						<li><a href="/docs/one_page_resume.pdf">CV</a></li>
					</ul>
				</nav>
			</header>

		<!-- Banner -->
			<section id="banner">
				<h2>Ian Osband</h2>
				<p>Data, decisions and learning</p>
			</section>

        <!-- About me -->
        <section id="about" class="wrapper style1">
            <div class="container 75%">
                <div class="row 200%">
                    <div class="6u 12u$(medium)">
                        <header class="major">
                            <h2>About me</h2>
                            <div class="11u"><span class="image fit"><img src="images/ian_face.jpeg" style="max-width: 400px" alt="Ian Osband" /></span></div>
                        </header>
                    </div>
                    <div class="6u$ 12u$(medium)">
                        <br>
                        <p>I am a researcher at <a href="https://openai.com" target="_blank" rel="noopener noreferrer">OpenAI</a> working to solve artificial intelligence.
                        My research focus is on decision making under uncertainty, aka <em>reinforcement learning</em>.
                        I am particularly interested in the problem of exploration: how to learn to make good decisions when you are uncertain about the world.
                        I use these insights to improve ChatGPT to develop the "data flywheel". You can learn more in this <a href="https://www.talkrl.com/episodes/ian-osband" target="_blank" rel="noopener noreferrer">podcast</a>.
                        </p>

                        <p>Before joining OpenAI, I was a staff research scientist at <a href="https://deepmind.google.com" target="_blank" rel="noopener noreferrer">Google DeepMind</a>, initially in London, and then moving to SF to help build the "efficient agent team".
                        I completed my Ph.D. at <a href="https://engineering.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford University</a> advised by <a href="http://web.stanford.edu/~bvr/" target="_blank" rel="noopener noreferrer">Benjamin Van Roy</a>.
                        My thesis <a href="https://searchworks.stanford.edu/view/11891201" target="_blank" rel="noopener noreferrer">Deep Exploration via Randomized Value Functions</a> won second place in the national <a href="https://www.informs.org/Recognizing-Excellence/INFORMS-Prizes/George-B.-Dantzig-Dissertation-Award" target="_blank" rel="noopener noreferrer">Dantzig dissertation award</a>. 
                        </p>

                        <p>Before coming to Stanford, I studied maths at <a href="https://www.maths.ox.ac.uk/" target="_blank" rel="noopener noreferrer">Oxford University</a>, and worked for <a href="https://www.jpmorgan.com/pages/jpmorgan" target="_blank" rel="noopener noreferrer">J.P.Morgan</a> as a credit derivatives strategist in London and New York.
                        If you want to get the latest scoop then check out my <a href="https://twitter.com/ianosband?lang=en" target="_blank" rel="noopener noreferrer">twitter</a>.
                        </p>

                    </div>
                </div>
            </div>
        </section>



			<!-- Research -->
				<section id="research" class="wrapper style2 special">
					<div class="container">
						<header class="major special">
							<h2>Research Highlights</h2>
							<p>Quick links and catchy taglines</p>
						</header>
						<div class="feature-grid">
              <div class="feature-grid">
<!--                 <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1608.02731">
                          <img src="images/endless-railroad.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Posterior Sampling for Reinforcement Learning without Episodes</h4>
                          <p>Technical note 2016</p>
                      </header>
                      <p>The <a href="https://arxiv.org/abs/1406.3926" target="_blank">previously published results</a> for posterior sampling without episodic reset are incorrect. This note clarifies some of the issues in this space.</p>
                  </div>
                </div>                <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1608.02732">
                          <img src="images/force_bound.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>On Lower Bounds for Regret in Reinforcement Learning</h4>
                          <p>Technical note 2016</p>
                      </header>
                      <p>The <a href="http://www.stat.berkeley.edu/~bartlett/papers/bt-rarbarllwcm-09.pdf" target="_blank">previously published proof</a> for the lower bound of any reinforcement learning algorithm are incorrect. This note clarifies some of the issues in this space.</p>
                  </div>
                </div> -->
              <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1703.07608">
                          <img src="images/bottomless_pit.png" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Deep Exploration via Randomized Value Functions</h4>
                          <p>JMLR 2018 (Accepted)</p>
                      </header>
                      <p>Journal paper that embodies the best pieces of research from my PhD and other pieces of work from our group... my favorite paper!
                      </p>
                  </div>
                </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf">
                          <img src="images/dice.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>A Tutorial on Thompson Sampling</h4>
                          <p>Foundations and Trends in Machine Learning 2018</p>
                      </header>
                      <p>A really nice tutorial on Thompson sampling: what it is, why it works and when to use it.
                      Includes lots of examples (+ <a href="https://github.com/iosband/ts_tutorial" target="_blank">code</a>).
                      Focus on building an intuition, rather than getting bogged down in theorems.
                      </p>
                  </div>
                </div>
<!-------------------------------------------------------------------------------------------->
              <div class="feature">
                  <div class="image rounded">
                      <a href="https://bit.ly/rpf_nips">
                          <img src="images/prior_fun.png" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Randomized Prior Functions for Deep Reinforcement Learning</h4>
                          <p>NeurIPS 2018 (Spotlight)</p>
                      </header>
                      <p>Add a "prior effect" to your bootstrap posterior with one simple trick: add a random function offset to each neural net in your ensemble!
                      </p>
                  </div>
                </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf">
                          <img src="images/concurrent_robot.png" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Scalable Coordinated Exploration in Concurrent Reinforcement Learning</h4>
                          <p>NeurIPS 2018</p>
                      </header>
                      <p>If you have a team of agents, rather than just one, you need to coordinate them to explore efficiently.
                      Randomized value functions work well here... if you do it right...
                      </p>
                  </div>
                </div>
<!-------------------------------------------------------------------------------------------->
              <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1607.00215">
                          <img src="images/rect_conf.png" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Why is Posterior Sampling Better than Optimism for Reinforcement Learning?</h4>
                          <p>ICML 2016 (full oral), EWRL 2016</p>
                      </header>
                      <p>Computational results demonstrate that PSRL dramatically outperforms UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it.</p>
                  </div>
                </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="http://arxiv.org/abs/1602.04621">
                          <img src="images/pacman.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Deep Exploration via Bootstrapped Deep Q-Networks</h4>
                          <p>NeurIPS 2016</p>
                      </header>
                      <p>Deep exploration and deep reinforcement learning. Takes the insight from efficient exploration via randomized value functions and attains state of the art results on Atari. Includes some sweet vids.</p>
                  </div>
                </div>
<!-- 			<div class="feature">
					<div class="image rounded">
                        <a href="http://xxx.tau.ac.il/abs/1507.00300">
                            <img src="images/deep.jpg" alt="" />
                        </a>
                    </div>
					<div class="content">
						<header>
							<h4>Bootstrapped Thompson Sampling and Deep Exploration</h4>
							<p>Technical pre-print 2015</p>
						</header>
						<p>A principled approach to efficient exploration with generalization that can be implemented for deep learning models at scale. Use an augmented bootstrap to approximate the posterior distribution.</p>
					</div>
				</div> -->
							<div class="feature">
								<div class="image rounded">
                                    <a href="http://arxiv.org/abs/1402.0635">
                                    <img src="images/dog.png" alt="" />
                                    </a>
                                </div>
								<div class="content">
									<header>
										<h4>Generalization and Exploration via Randomized Value functions</h4>
										<p>ICML 2016</p>
									</header>
									<p>You can combine efficient exploration and generalization, all without a model-based planning step. Some cool empirical results and also some theory.</p>
								</div>
							</div>

                <div class="feature">
                    <div class="image rounded">
                        <a href="http://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension">
                        <img src="images/eluder.jpg" alt="" />
                        </a>
                    </div>
                    <div class="content">
                        <header>
                            <h4>Model-based Reinforcement Learning and the Eluder Dimension</h4>
                            <p>NeurIPS 2014</p>
                        </header>
                        <p>The first general analysis of model based RL in terms of the dimensionality, rather than the cardinality, of the system. Several new state of the art results including linear systems.</p>
                    </div>
                </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="http://papers.nips.cc/paper/5445-near-optimal-reinforcement-learning-in-factored-mdps">
                      <img src="images/factored.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Near-optimal Reinforcement Learning in Factored MDPs</h4>
                          <p>NeurIPS 2014 (Spotlight), INFORMS 2014</p>
                      </header>
                      <p>If the environment is a structured graph (aka factored MDP), then you can exploit that to learn quickly. You can adapt UCB-style approaches for this, posterior sampling gets it for free. </p>
                  </div>
              </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling">
                      <img src="images/dice.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>(More) Efficient Reinforcement Learning via Posterior Sampling</h4>
                          <p>NeurIPS 2013, RLDM 2013</p>
                      </header>
                      <p>You don't need to use loose UCB-style algorithms to get regret bounds for reinforcement learning. Posterior sampling is more efficient in terms of computation and data and shares similar gaurantees. </p>
                  </div>
              </div>
			</div>
		</div>
	</section>

        <!-- teaching -->
            <section id="teaching" class="wrapper">
                <div class="container" style="max-width: 1000px; line-height: 140%;">

                    <header class="major special">
                        <h2>Teaching</h2>
                        <p>Past courses</p>
                    </header>

        <p id=""><a href="http://explorecourses.stanford.edu/search;jsessionid=eqe6sam4fpd614wv5fgcsffa2?q=MS%26E+142%3A+Introductory+Financial+Analysis&filter-coursestatus-Active=on&view=catalog&academicYear=20142015" target="_blank"><span class="image left"><img src="images/stocks.jpg" title="" alt="" style="max-width: 140px"></span></a><h4> MS&ampE 145 - Introduction to Financial analysis - lead instructor</h4><p>Finance for engineers aimed for over 75 undergraduate juniors and seniors. Everything from the time value of money to CAPM to elementary option pricing and portfolio optimization. Practical data analysis skills taught through spreadsheets.</p>

            <br>
            <br>

       <p id=""><a href="https://web.stanford.edu/class/msande338/" target="_blank"><span class="image left"><img src="images/robots.jpg" title="" alt="" style="max-width: 140px"></span></a><h4> MS&ampE 338 - Reinforcement learning - assistant instructor</h4><p>An advanced PhD level course aimed at graduate students looking to engage in research. I managed the class research projects and gave several lectures throughout the course.</p>

                </div>
            </section>


			<!-- Four -->
				<section id="resume" class="wrapper style3 special">
					<div class="container">
						<header class="major">
							<h2>Want more?</h2>
							<p>Hit me up at any of the links below. </p>
                            <p> <a href="docs/one_page_resume.pdf" target="_blank"> Here </a> is a copy of my CV. </p>
						</header>
<!-- 						<ul class="actions">
							<li><a href="#" class="button special big">Get in touch</a></li>
						</ul> -->
					</div>
				</section>

		<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<ul class="icons">
                        <li><a href="mailto:ian.osband@gmail.com" class="icon fa-mail-reply"></a></li>
						<li><a href="https://www.facebook.com/ian.osband" class="icon fa-facebook"></a></li>
						<li><a href="https://twitter.com/ianosband?lang=en" class="icon fa-twitter"></a></li>
						<li><a href="https://instagram.com/ianstagrammies/" class="icon fa-instagram"></a></li>
                        <li><a href="https://github.com/iosband" class="icon fa-github"></a></li>
                        <li><a href="https://bitbucket.org/iosband/" class="icon fa-bitbucket"></a></li>
                        <li><a href="https://plus.google.com/u/0/+IanOsband" class="icon fa-google-plus"></a></li>
                        <li><a href="https://www.linkedin.com/in/iosband" class="icon fa-linkedin"></a></li>
					</ul>
					<ul class="copyright">
						<li>Copyright &copy; Ian Osband</li>
                        <li> HTML5</li>
						<li>Design: <a href="http://templated.co">TEMPLATED</a></li>
					</ul>
				</div>
			</footer>

	</body>
</html>

