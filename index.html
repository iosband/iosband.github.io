<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Ian Osband - Homepage</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>

        <!-- MathJax options -->
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true
            }
          });
        </script>

        <!-- Google tracking -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-65368616-1', 'auto');
          ga('send', 'pageview');
        </script>

        <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt">
          <h1><strong><a href="/index.html">Stanford</a></strong> Machine Learning<br>
          <!-- <strong><a href="/index.html">Google</a></strong> Deepmind -->
          </h1>
				<nav id="nav">
					<ul>
						<li><a href="/">Home</a></li>
            <li><a href="/research.html">Research</a></li>
            <li><a href="/blog.html">Blog</a></li>
						<li><a href="/docs/IanOsband_Academic.pdf">CV</a></li>
					</ul>
				</nav>
			</header>

		<!-- Banner -->
			<section id="banner">
				<h2>Ian Osband</h2>
				<p>Data, decisions and learning</p>
			</section>

			<!-- About me -->
				<section id="about" class="wrapper style1">
					<div class="container 75%">
						<div class="row 200%">
							<div class="6u 12u$(medium)">
								<header class="major">
									<h2>About me</h2>
									<div class="11u"><span class="image fit"><img src="images/tie.jpg" style="max-width: 300px" alt="" /></span></div>
								</header>
							</div>
							<div class="6u$ 12u$(medium)">
                                <br>
								<p>I am a research scientist at  PhD student at <a href="https://engineering.stanford.edu/" target="_blank">Google Deepmind</a> working to solve artificial intelligence.
                My research focus is on decision making under uncertainty, which is often called reinforcement learning.
                I want to design autonomous agents that teach themselves to do well in any task. If we can do this, then we will be well on our way to general <a href="https://en.wikipedia.org/wiki/Artificial_intelligence" target="_blank">AI</a>.
                </p>

                <p>I completed my Ph.D. at <a href="https://engineering.stanford.edu/" target="_blank">Stanford University</a> advised by <a href="http://web.stanford.edu/~bvr/" target="_blank">Benjamin Van Roy</a>.
                My thesis "Deep Exploration via Randomized Value Functions" takes some of the first steps towards a practical reinforcement learning algorithm that combines efficient generalization and exploration.
                This is crucial for any algorithm to be simultaneously computationally and statistically efficient in complex domains.
                </p>

                <p>Before coming to Stanford I studied maths at <a href="https://www.maths.ox.ac.uk/" target="_blank">Oxford University</a> and worked for <a href="https://www.jpmorgan.com/pages/jpmorgan" target="_blank">J.P.Morgan</a> as a credit derivatives strategist.
                I spent the summer of 2015 working for <a href="https://www.google.com/about/careers/" target="_blank">Google</a> in Mountain View and, after a great internship in 2016, I have joined <a href="http://deepmind.com/" target="_blank">DeepMind</a> full time in London. </p>

                <p>
                If you want to know what I'm thinking check out my <a href="/blog.html">blog</a>.
                </p>
							</div>
						</div>
					</div>
				</section>



			<!-- Research -->
				<section id="research" class="wrapper style2 special">
					<div class="container">
						<header class="major special">
							<h2>Research Highlights</h2>
							<p>Quick links and catchy taglines</p>
						</header>
						<div class="feature-grid">
              <div class="feature-grid">
<!--                 <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1608.02731">
                          <img src="images/endless-railroad.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Posterior Sampling for Reinforcement Learning without Episodes</h4>
                          <p>Technical note 2016</p>
                      </header>
                      <p>The <a href="https://arxiv.org/abs/1406.3926" target="_blank">previously published results</a> for posterior sampling without episodic reset are incorrect. This note clarifies some of the issues in this space.</p>
                  </div>
                </div>                <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1608.02732">
                          <img src="images/force_bound.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>On Lower Bounds for Regret in Reinforcement Learning</h4>
                          <p>Technical note 2016</p>
                      </header>
                      <p>The <a href="http://www.stat.berkeley.edu/~bartlett/papers/bt-rarbarllwcm-09.pdf" target="_blank">previously published proof</a> for the lower bound of any reinforcement learning algorithm are incorrect. This note clarifies some of the issues in this space.</p>
                  </div>
                </div> -->
              <div class="feature">
                  <div class="image rounded">
                      <a href="https://arxiv.org/abs/1607.00215">
                          <img src="images/rect_conf.png" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Why is Posterior Sampling Better than Optimism for Reinforcement Learning?</h4>
                          <p>In submission</p>
                      </header>
                      <p>Computational results demonstrate that PSRL dramatically outperforms UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it.</p>
                  </div>
                </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="http://arxiv.org/abs/1602.04621">
                          <img src="images/pacman.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Deep Exploration via Bootstrapped Deep Q-Networks</h4>
                          <p>NIPS 2016</p>
                      </header>
                      <p>Deep exploration and deep reinforcement learning. Takes the insight from efficient exploration via randomized value functions and attains state of the art results on Atari. Includes some sweet vids.</p>
                  </div>
                </div>
<!-- 							<div class="feature">
								<div class="image rounded">
                                    <a href="http://xxx.tau.ac.il/abs/1507.00300">
                                        <img src="images/deep.jpg" alt="" />
                                    </a>
                                </div>
								<div class="content">
									<header>
										<h4>Bootstrapped Thompson Sampling and Deep Exploration</h4>
										<p>Technical pre-print 2015</p>
									</header>
									<p>A principled approach to efficient exploration with generalization that can be implemented for deep learning models at scale. Use an augmented bootstrap to approximate the posterior distribution.</p>
								</div>
							</div> -->
							<div class="feature">
								<div class="image rounded">
                                    <a href="http://arxiv.org/abs/1402.0635">
                                    <img src="images/dog.jpg" alt="" />
                                    </a>
                                </div>
								<div class="content">
									<header>
										<h4>Generalization and Exploration via Randomized Value functions</h4>
										<p>ICML 2016</p>
									</header>
									<p>You can combine efficient exploration and generalization, all without a model-based planning step. Some cool empirical results and also some theory. My favorite paper.</p>
								</div>
							</div>
                <div class="feature">
                    <div class="image rounded">
                        <a href="http://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension">
                        <img src="images/eluder.jpg" alt="" />
                        </a>
                    </div>
                    <div class="content">
                        <header>
                            <h4>Model based reinforcement learning and the eluder dimension</h4>
                            <p>NIPS 2014</p>
                        </header>
                        <p>The first general analysis of model based RL in terms of the dimensionality, rather than the cardinality, of the system. Several new state of the art results including linear systems.</p>
                    </div>
                </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="http://papers.nips.cc/paper/5445-near-optimal-reinforcement-learning-in-factored-mdps">
                      <img src="images/factored.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>Near-optimal Reinforcement Learning in Factored MDPs</h4>
                          <p>NIPS Spotlight 2014, INFORMS 2014</p>
                      </header>
                      <p>If the environment is a structured graph (aka factored MDP), then you can exploit that to learn quickly. You can adapt UCB-style approaches for this, posterior sampling gets it for free. </p>
                  </div>
              </div>
              <div class="feature">
                  <div class="image rounded">
                      <a href="http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling">
                      <img src="images/dice.jpg" alt="" />
                      </a>
                  </div>
                  <div class="content">
                      <header>
                          <h4>(More) Efficient Reinforcement Learning via Posterior Sampling</h4>
                          <p>NIPS 2013, RLDM 2013</p>
                      </header>
                      <p>You don't need to use loose UCB-style algorithms to get regret bounds for reinforcement learning. Posterior sampling is more efficient in terms of computation and data and shares similar gaurantees. </p>
                  </div>
              </div>
						</div>
					</div>
				</section>

        <!-- teaching -->
            <section id="teaching" class="wrapper">
                <div class="container" style="max-width: 1000px; line-height: 140%;">

                    <header class="major special">
                        <h2>Teaching</h2>
                        <p>Past courses</p>
                    </header>

        <p id=""><a href="http://explorecourses.stanford.edu/search;jsessionid=eqe6sam4fpd614wv5fgcsffa2?q=MS%26E+142%3A+Introductory+Financial+Analysis&filter-coursestatus-Active=on&view=catalog&academicYear=20142015" target="_blank"><span class="image left"><img src="images/stocks.jpg" title="" alt="" style="max-width: 140px"></span></a><h4> MS&ampE 145 - Introduction to Financial analysis - lead instructor</h4><p>Finance for engineers aimed for over 75 undergraduate juniors and seniors. Everything from the time value of money to CAPM to elementary option pricing and portfolio optimization. Practical data analysis skills taught through spreadsheets.</p>

            <br>
            <br>

       <p id=""><a href="https://web.stanford.edu/class/msande338/" target="_blank"><span class="image left"><img src="images/robots.jpg" title="" alt="" style="max-width: 140px"></span></a><h4> MS&ampE 338 - Reinforcement learning - assistant instructor</h4><p>An advanced PhD level course aimed at graduate students looking to engage in research. I managed the class research projects and gave several lectures throughout the course.</p>

                </div>
            </section>


			<!-- Four -->
				<section id="resume" class="wrapper style3 special">
					<div class="container">
						<header class="major">
							<h2>Want more?</h2>
							<p>Hit me up at any of the links below. </p>
                            <p> <a href="docs/IanOsband_Academic.pdf" target="_blank"> Here </a> is a copy of my CV. </p>
						</header>
<!-- 						<ul class="actions">
							<li><a href="#" class="button special big">Get in touch</a></li>
						</ul> -->
					</div>
				</section>

		<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<ul class="icons">
                        <li><a href="mailto:ian.osband@gmail.com" class="icon fa-mail-reply"></a></li>
						<li><a href="https://www.facebook.com/ian.osband" class="icon fa-facebook"></a></li>
						<!-- <li><a href="#" class="icon fa-twitter"></a></li> -->
						<li><a href="https://instagram.com/ianosband/" class="icon fa-instagram"></a></li>
                        <li><a href="https://github.com/iosband" class="icon fa-github"></a></li>
                        <li><a href="https://bitbucket.org/iosband/" class="icon fa-bitbucket"></a></li>
                        <li><a href="https://plus.google.com/u/0/+IanOsband" class="icon fa-google-plus"></a></li>
                        <li><a href="https://www.linkedin.com/in/iosband" class="icon fa-linkedin"></a></li>
					</ul>
					<ul class="copyright">
						<li>Copyright &copy; Ian Osband</li>
                        <li> HTML5</li>
						<li>Design: <a href="http://templated.co">TEMPLATED</a></li>
					</ul>
				</div>
			</footer>

	</body>
</html>

