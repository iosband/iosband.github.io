<!DOCTYPE HTML>
<!--
    Spatial by TEMPLATED
    templated.co @templatedco
    Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
    <head>
        <title>Ian Osband - Research</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
        <script src="js/jquery.min.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-layers.min.js"></script>
        <script src="js/init.js"></script>

        <!-- MathJax options -->
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true
            }
          });
        </script>

        <!-- Google tracking -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-65368616-1', 'auto');
          ga('send', 'pageview');
        </script>

        <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

        <noscript>
            <link rel="stylesheet" href="css/skel.css" />
            <link rel="stylesheet" href="css/style.css" />
            <link rel="stylesheet" href="css/style-xlarge.css" />
        </noscript>
    </head>
    <body class="landing">

        <!-- Header -->
            <header id="header" class="alt">
          <h1><strong><a href="/index.html">Stanford</a></strong> Machine Learning<br>
          <!-- <strong><a href="/index.html">Google</a></strong> Deepmind -->
          </h1>
                <nav id="nav">
                    <ul>
                        <li><a href="/">Home</a></li>
            <li><a href="/research.html">Research</a></li>
            <li><a href="/blog.html">Blog</a></li>
                        <li><a href="/docs/IanOsband_Academic.pdf">CV</a></li>
                    </ul>
                </nav>
            </header>

        <!-- Banner -->
            <section id="banner">
                <h2>Ian Osband</h2>
                <p>Data, decisions and learning</p>
            </section>

<!--------------------------------------------------------------------------------------------------------- -->
<!-- interests -->
<section id="interests" class="wrapper style1">
  <div class="container 75%">
    <div class="row 200%">
      <div class="6u 12u$(medium)">
        <header class="major special">
            <h2>Research Focus</h2>
            <p>Statistically and computationally efficient reinforcement learning</p>
        </header>
    </div>
    <div class="6u$ 12u$(medium)">
<p>If you want to make good decisions from data, you need good data.
Traditional statistics and machine learning has made great progress in learning form fixed datasets, but even an <i>optimal</i> learning algorithm for learning from a fixed dataset can be arbitrarily bad when the decisions it makes affect the data it gets.
I'm trying to design algorithms which can learn to take good actions (and so may affect its environment) in a manner which is simultaneously
<a href="https://en.wikipedia.org/wiki/Computational_complexity_theory" target="_blank">computationally tractable</a>
and
<a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)statistically efficient." target="_blank">statistically efficient</a>.
</p>
</div>
</div>
</div>
</section>


<!--------------------------------------------------------------------------------------------------------- -->
    <section id="publications" class="wrapper style2">
        <div class="container" style="max-width: 1000px; line-height: 140%;">

            <header class="major special">
                <h2>Publications</h2>
                <p>All the links you could want</p>
            </header>

<p id="Osband2016Thesis">
<h4>
    PhD Thesis: Deep Exploration via Randomized Value Functions
     <a href="research.html#Osband2016Thesis"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="https://searchworks.stanford.edu/view/11891201" target="_blank">
    <span class="image left">
        <img src="images/phd_hat.jpg" title="Deep Exploration via Randomized Value Functions" alt="Deep Exploration via Randomized Value Functions" style="max-width: 160px">
    </span>
</a>
Statistically efficient RL requires "deep exploration".
Previous approaches to deep exploration have not been computationally tractable beyond small scale problems.
This dissertation presents an alternative approach through the use of randomized value functions.
<br><br>
Ian Osband<br>
<i>PhD Thesis 2016</i><br>
[<a href="https://searchworks.stanford.edu/view/11891201" target="_blank">Stanford link</a>]
[<a href="http://iosband.github.io/docs/iosband_thesis.pdf" target="_blank">Mirror</a>]
[<a href="https://www.youtube.com/watch?v=ck4GixLs4ZQ" target="_blank">YouTube</a>]
</p>
<br>


<p id="Osband2016PSRLepisodes">
<h4>
    Posterior Sampling for Reinforcement Learning without Episodes
     <a href="research.html#Osband2016PSRLepisodes"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="https://arxiv.org/abs/1608.02731" target="_blank">
    <span class="image left">
        <img src="images/endless-railroad.jpg" title="Posterior Sampling for Reinforcement Learning without Episodes" alt="Posterior Sampling for Reinforcement Learning without Episodes" style="max-width: 160px">
    </span>
</a>
Some of the <a href="https://arxiv.org/abs/1406.3926" target="_blank">previously published results</a> for posterior sampling without episodic reset are incorrect.
This note clarifies some of the issues in this space and presents some conjectures towards future solutions.
<br><br>
Ian Osband, Benjamin Van Roy<br>
<i>arXiv technical pre-print 2016</i><br>
[<a href="https://arxiv.org/abs/1608.02731" target="_blank">Paper</a>]
</p>
<br>


<p id="Osband2016lower">
<h4>
    On Lower Bounds for Regret in Reinforcement Learning
     <a href="research.html#Osband2016lower"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="https://arxiv.org/abs/1608.02732" target="_blank">
    <span class="image left">
        <img src="images/force_bound.jpg" title="On Lower Bounds for Regret in Reinforcement Learning" alt="On Lower Bounds for Regret in Reinforcement Learning" style="max-width: 160px">
    </span>
</a>
A <a href="http://www.stat.berkeley.edu/~bartlett/papers/bt-rarbarllwcm-09.pdf" target="_blank">previously published proof</a> for the lower bounds on what is possible for any reinforcement learning algorithm are incorrect.
posterior sampling without episodic reset is incorrect.
This note clarifies some of the issues in this space and presents some further conjectures on what might be true in this space.
<br><br>
Ian Osband, Benjamin Van Roy<br>
<i>arXiv technical pre-print 2016</i><br>
[<a href="https://arxiv.org/abs/1608.02732" target="_blank">Paper</a>]
</p>
<br>


<p id="Osband2016why">
<h4>
    Why is Posterior Sampling Better than Optimism for Reinforcement Learning?
     <a href="research.html#Osband2016why"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="https://arxiv.org/abs/1607.00215" target="_blank">
    <span class="image left">
        <img src="images/rect_conf_2.png" title="Why is Posterior Sampling Better than Optimism for Reinforcement Learning?" alt="Why is Posterior Sampling Better than Optimism for Reinforcement Learning?" style="max-width: 160px">
    </span>
</a>
Computational results demonstrate that PSRL dramatically outperforms UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it.
<br><br>
Ian Osband, Benjamin Van Roy<br>
<i>EWRL 2016 (full oral)</i><br>
[<a href="https://arxiv.org/abs/1607.00215" target="_blank">Paper</a>]
[<a href="https://github.com/iosband/TabulaRL" target="_blank">Code</a>]
</p>
<br>


<p id="Osband2016deep">
<h4>
    Deep Exploration via Bootstrapped DQN
     <a href="research.html#Osband2016deep"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="https://arxiv.org/abs/1602.04621" target="_blank">
    <span class="image left">
        <img src="images/pacman.jpg" title="Deep Exploration via Bootstrapped DQN" alt="Deep Exploration via Bootstrapped DQN" style="max-width: 160px">
    </span>
</a>
Deep exploration and deep reinforcement learning. Takes the insight from efficient exploration via randomized value functions and attains state of the art results on Atari. Includes some sweet vids.
<br><br>
Ian Osband, Charles Blundell, Alex Pritzel, Benjamin Van Roy<br>
<i>NIPS 2016</i><br>
[<a href="https://arxiv.org/abs/1602.04621" target="_blank">Paper</a>]
[<a href="https://www.youtube.com/playlist?list=PLdy8eRAW78uLDPNo1jRv8jdTx7aup1ujM" target="_blank">Videos</a>]
</p>
<br>


<p id="Osband2015bootstrapped">
<h4>
    Bootstrapped Thompson Sampling and Deep Exploration
     <a href="research.html#Osband2015bootstrapped"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="https://arxiv.org/abs/1507.00300" target="_blank">
    <span class="image left">
        <img src="images/deep.jpg" title="Bootstrapped Thompson Sampling and Deep Exploration" alt="Bootstrapped Thompson Sampling and Deep Exploration" style="max-width: 160px">
    </span>
</a>
A principled approach to efficient exploration with generalization that can be implemented for deep learning models at scale. Use an augmented bootstrap to approximate the posterior distribution.
<br><br>
Ian Osband, Benjamin Van Roy<br>
<i>arXiv technical pre-print 2015</i><br>
[<a href="https://arxiv.org/abs/1507.00300" target="_blank">Paper</a>]
</p>
<br>

<p id="Osband2015generalization">
<h4>
    Generalization and Exploration via Randomized Value functions
     <a href="research.html#Osband2015generalization"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>

<a href="https://arxiv.org/abs/1402.0635" target="_blank">
    <span class="image left">
        <img src="images/dog.jpg" title="Generalization and Exploration via Randomized Value functions" alt="Generalization and Exploration via Randomized Value functions" style="max-width: 160px">
    </span>
</a>
You can combine efficient exploration and generalization, all without a model-based planning step. Some cool empirical results and also some theory. My favorite paper.
<br><br>
Ian Osband, Zheng Wen, Benjamin Van Roy<br>
<i>ICML 2016</i><br>
[<a href="https://arxiv.org/abs/1402.0635" target="_blank">Paper</a>]
[<a href="http://techtalks.tv/talks/generalization-and-exploration-via-randomized-value-functions/62467/" target="_blank">Talk</a>]
[<a href="docs/ICML2016_RLSVI_Poster.pdf" target="_blank">Poster</a>]
[<a href="https://docs.google.com/presentation/d/1OkJjM1EGEHbcJM--98hWTc9fZdlMnegY8NFUToYcrLQ/edit?usp=sharing" target="_blank">Slides</a>]

</p>
<br>


<p id="Osband2014model">
<h4>
    Model-based Reinforcement Learning and the Eluder Dimension
     <a href="research.html#Osband2014model"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="http://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension" target="_blank">
    <span class="image left">
        <img src="images/eluder.jpg" title="Model-based Reinforcement Learning and the Eluder Dimension" alt="Model-based Reinforcement Learning and the Eluder Dimension" style="max-width: 160px">
    </span>
</a>
The first general analysis of model based RL in terms of the dimensionality, rather than the cardinality, of the system. Several new state of the art results including linear systems.
<br><br>
Ian Osband, Benjamin Van Roy<br>
<i>NIPS 2014</i><br>
[<a href="http://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension" target="_blank">Paper</a>]
[<a href="docs/eluder_mdp_poster.pdf" target="_blank">Poster</a>]
</p>
<br>


<p id="Osband2014factored">
<h4>
    Near-optimal Reinforcement Learning in Factored MDPs
     <a href="research.html#Osband2014factored"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>
<a href="http://papers.nips.cc/paper/5445-near-optimal-reinforcement-learning-in-factored-mdps" target="_blank">
    <span class="image left">
        <img src="images/factored.jpg" title="Near-optimal Reinforcement Learning in Factored MDPs" alt="Near-optimal Reinforcement Learning in Factored MDPs" style="max-width: 160px">
    </span>
</a>
If the environment is a structured graph (aka factored MDP), then you can exploit that to learn quickly. You can adapt UCB-style approaches for this, posterior sampling gets it for free.
<br><br>
Ian Osband, Benjamin Van Roy<br>
<i>NIPS 2014 (spotlight), INFORMS 2014</i><br>
[<a href="http://papers.nips.cc/paper/5445-near-optimal-reinforcement-learning-in-factored-mdps" target="_blank">Paper</a>]
[<a href="docs/factored_mdp_poster.pdf" target="_blank">Poster</a>]
[<a href="docs/factored_spot_nips_v2.pdf" target="_blank">Spotlight</a>]
</p>
<br>

<p id="Osband2013more">
<h4>
    (More) Efficient Reinforcement Learning via Posterior Sampling
     <a href="research.html#Osband2013more"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>

<a href="http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling" target="_blank">
    <span class="image left">
        <img src="images/dice.jpg" title="(More) Efficient Reinforcement Learning via Posterior Sampling" alt="(More) Efficient Reinforcement Learning via Posterior Sampling" style="max-width: 160px">
    </span>
</a>
You don't need to use loose UCB-style algorithms to get regret bounds for reinforcement learning. Posterior sampling is more efficient in terms of computation and data and shares similar gaurantees.
<br><br>
Ian Osband, Dan Russo, Benjamin Van Roy<br>
<i>NIPS 2013, RLDM 2013</i><br>
[<a href="http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling" target="_blank">Paper</a>]
[<a href="docs/psrl_poster_2013.pdf" target="_blank">Poster</a>]
</p>
<br>

<p id="Osband2012deep">
<h4>
    Deep Learning for Time Series Modeling
     <a href="research.html#Osband2012deep"><span class="image right"><img src="images/link-to-here.png" title="Link to this paper" alt="Link to this paper" style="max-width: 15px"></span></a>
</h4>

<a href="http://cs229.stanford.edu/proj2012/BussetiOsbandWong-DeepLearningForTimeSeriesModeling.pdf" target="_blank">
    <span class="image left">
        <img src="images/stocks_square.jpg" title="Deep Learning for Time Series Modeling" alt="Deep Learning for Time Series Modeling" style="max-width: 160px">
    </span>
</a>
We apply deep learning techniques to energy load forecasting across 20 geographic regions.
We found that recurrent network architectures were particularly suited to this task.
Class project for CS 229 in my first quarter at Stanford.
<br><br>
Ian Osband, Enso Busseti, Scott Wong<br>
<i>Class project 2012</i><br>
[<a href="http://cs229.stanford.edu/proj2012/BussetiOsbandWong-DeepLearningForTimeSeriesModeling.pdf" target="_blank">Paper</a>]
</p>
<br>


<!--------------------------------------------------------------------------------------------------------- -->

</div>
</section>


            <!-- Four -->
                <section id="resume" class="wrapper style3 special">
                    <div class="container">
                        <header class="major">
                            <h2>Want more?</h2>
                            <p>Hit me up at any of the links below. </p>
                            <p> <a href="docs/IanOsband_Academic.pdf" target="_blank"> Here </a> is a copy of my CV. </p>
                        </header>
<!--                        <ul class="actions">
                            <li><a href="#" class="button special big">Get in touch</a></li>
                        </ul> -->
                    </div>
                </section>

        <!-- Footer -->
            <footer id="footer">
                <div class="container">
                    <ul class="icons">
                        <li><a href="mailto:ian.osband@gmail.com" class="icon fa-mail-reply"></a></li>
                        <li><a href="https://www.facebook.com/ian.osband" class="icon fa-facebook"></a></li>
                        <!-- <li><a href="#" class="icon fa-twitter"></a></li> -->
                        <li><a href="https://instagram.com/ianosband/" class="icon fa-instagram"></a></li>
                        <li><a href="https://github.com/iosband" class="icon fa-github"></a></li>
                        <li><a href="https://bitbucket.org/iosband/" class="icon fa-bitbucket"></a></li>
                        <li><a href="https://plus.google.com/u/0/+IanOsband" class="icon fa-google-plus"></a></li>
                        <li><a href="https://www.linkedin.com/in/iosband" class="icon fa-linkedin"></a></li>
                    </ul>
                    <ul class="copyright">
                        <li>Copyright &copy; Ian Osband</li>
                        <li> HTML5</li>
                        <li>Design: <a href="http://templated.co">TEMPLATED</a></li>
                    </ul>
                </div>
            </footer>

    </body>
</html>

